{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXdanYPhryaX8+qrd/JXR7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RK05Zyth3kM_"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    {\n",
        "        \"question\": \"List the advantages of pre-training and fine-tuning.\",\n",
        "        \"context\": \"Lecture 8 slide 39\",\n",
        "        \"answer\": \"Applying pre-training and fine-tuning leverages underlying information, reduces reliance on task-specific labeled data, initializes model parameters for more generalizable NLP applications, saves training costs, and provides robust representation of language contexts.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"List measures of fit used to evaluate language models.\",\n",
        "        \"context\": \"Lecture 3 slide 15\",\n",
        "        \"answer\": \"Likelihood, (negative) log likelihood, and perplexity are measures of fit for evaluating LMs.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are convolutional neural networks?\",\n",
        "        \"context\": \"Lecture 16 slide 10\",\n",
        "        \"answer\": \"CNNs are models that extract spatial and temporal features from images using convolution operations, with pooling layers reducing dimensionality for computational efficiency.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the attention method?\",\n",
        "        \"context\": \"Lecture 5 slides 45-46\",\n",
        "        \"answer\": \"The attention method looks at all input tokens at the same time to decide which ones are the most important to decide the next token. This method treats each token's representation as a query to select information from a set of values.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is needed to achieve a more general world model for simulative reasoning?\",\n",
        "        \"context\": \"Guest lecture 1 slide 51\",\n",
        "        \"answer\": \"A more general world model requires integrating different spaces for simulation/reasoning, generalist language and vision capabilities, and real-time control of the simulation through action inputs.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are the pros and cons of the encoder-decoder approach?\",\n",
        "        \"context\": \"Lecture 8 slide 67\",\n",
        "        \"answer\": \"The encoder-decoder approach balances bidirectional context use and open-text generation, making it effective for multi-task fine-tuning. However, it requires more text wrangling, is harder to train, and is less flexible for natural language generation.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is add-one estimation?\",\n",
        "        \"context\": \"Lecture 2 slide 64\",\n",
        "        \"answer\": \"Add-one estimation, which is also called Laplace smoothing, smooths probability calculations by adding one to all frequency counts.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are the traits of high-quality pre-training data, and where can it be found?\",\n",
        "        \"context\": \"Lecture 6 slide 88\",\n",
        "        \"answer\": \"High-quality pre-training data is natural, clean, informative, and diverse, and can be found in sources such as books, Wikipedia, news, and scientific papers.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Can you explain what tokenization means?\",\n",
        "        \"context\": \"Lecture 3 slide 28\",\n",
        "        \"answer\": \"Tokenization is the process of splitting large chunks of text into smaller pieces called tokens.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Why do LLMs need to use tools, and what do they use them for?\",\n",
        "        \"context\": \"Lecture 10 slide 32\",\n",
        "        \"answer\": \"LLMs use tools to aid with tasks beyond their ability. They mainly use tools for knowledge, symbolic, and external environment operations.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How much of my grade is based on participation?\",\n",
        "        \"context\": \"Lecture 1 slide 15\",\n",
        "        \"answer\": \"10% of your grade is determined by course participation/attendance, plus 3% extra credit for attending discussions.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Define ReAct prompting.\",\n",
        "        \"context\": \"Lecture 10 slide 17\",\n",
        "        \"answer\": \"ReAct prompting enables language models to generate reasoning traces and task-specific actions in an interleaved manner, improving interpretability and mitigating errors.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are content overlap metrics, and what are their strengths and weaknesses?\",\n",
        "        \"context\": \"Lecture 13 slides 78-80\",\n",
        "        \"answer\": \"Content overlap metrics involve computing a similarity score between generated and gold-standard text. They are fast and efficient to calculate, but are not ideal for open-ended tasks and they have no concept of semantic relatedness.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is an n-gram model?\",\n",
        "        \"context\": \"Lecture 2 slide 43\",\n",
        "        \"answer\": \"An n-gram model is a language model that determines the probability of a word from the previous n-1 words.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are the problems with human evaluations?\",\n",
        "        \"context\": \"Lecture 14 slide 20\",\n",
        "        \"answer\": \"Human evaluations are slow, expensive, inconsistent, irreproducible, and sometimes illogical, with risks of misinterpretation and potential reliance on LLMs by crowd workers.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is layer normalization, and how does it work?\",\n",
        "        \"context\": \"Lecture 6 slide 71\",\n",
        "        \"answer\": \"Layer normalization is a trick to help models learn faster by normalizing to unit mean and standard deviation in each layer, then modulating by learned parameters.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Tell me the pros and cons of one-hot encoding.\",\n",
        "        \"context\": \"Lecture 4 slide 36\",\n",
        "        \"answer\": \"One-hot encoding creates sparse representations without requiring learning, but the feature space must be the same size as the vocabulary and there is no way to encode shared representations of words.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is prompt-tuning?\",\n",
        "        \"context\": \"Lecture 8 slide 46\",\n",
        "        \"answer\": \"Prompt-tuning is contemporaneous work to pre-tuning, where a single \\\"soft prompt\\\" representation is prepended to the embedded input on the encoder side.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the time complexity of beam search?\",\n",
        "        \"context\": \"Lecture 12 slide 21\",\n",
        "        \"answer\": \"The time complexity of beam search is O(beam width * vocab size * generation length).\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are the advantages and applications of persona-based prompting?\",\n",
        "        \"context\": \"Lecture 11 slide 11\",\n",
        "        \"answer\": \"Persona-based prompting increases engagement and provides context-aware responses, with applications in recommendation systems, customer support, and specialized domains like medicine and law.\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "for prompt in prompts:\n",
        "    for key, value in prompt.items():\n",
        "        print(textwrap.fill(f\"{key}: {value}\", 120))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXQL_0Le5wBG",
        "outputId": "34ba9980-5cf6-4512-c8a5-888c7a5f35c1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question: List the advantages of pre-training and fine-tuning.\n",
            "context: Lecture 8 slide 39\n",
            "answer: Applying pre-training and fine-tuning leverages underlying information, reduces reliance on task-specific\n",
            "labeled data, initializes model parameters for more generalizable NLP applications, saves training costs, and provides\n",
            "robust representation of language contexts.\n",
            "\n",
            "question: List measures of fit used to evaluate language models.\n",
            "context: Lecture 3 slide 15\n",
            "answer: Likelihood, (negative) log likelihood, and perplexity are measures of fit for evaluating LMs.\n",
            "\n",
            "question: What are convolutional neural networks?\n",
            "context: Lecture 16 slide 10\n",
            "answer: CNNs are models that extract spatial and temporal features from images using convolution operations, with\n",
            "pooling layers reducing dimensionality for computational efficiency.\n",
            "\n",
            "question: What is the attention method?\n",
            "context: Lecture 5 slides 45-46\n",
            "answer: The attention method looks at all input tokens at the same time to decide which ones are the most important to\n",
            "decide the next token. This method treats each token's representation as a query to select information from a set of\n",
            "values.\n",
            "\n",
            "question: What is needed to achieve a more general world model for simulative reasoning?\n",
            "context: Guest lecture 1 slide 51\n",
            "answer: A more general world model requires integrating different spaces for simulation/reasoning, generalist language\n",
            "and vision capabilities, and real-time control of the simulation through action inputs.\n",
            "\n",
            "question: What are the pros and cons of the encoder-decoder approach?\n",
            "context: Lecture 8 slide 67\n",
            "answer: The encoder-decoder approach balances bidirectional context use and open-text generation, making it effective\n",
            "for multi-task fine-tuning. However, it requires more text wrangling, is harder to train, and is less flexible for\n",
            "natural language generation.\n",
            "\n",
            "question: What is add-one estimation?\n",
            "context: Lecture 2 slide 64\n",
            "answer: Add-one estimation, which is also called Laplace smoothing, smooths probability calculations by adding one to\n",
            "all frequency counts.\n",
            "\n",
            "question: What are the traits of high-quality pre-training data, and where can it be found?\n",
            "context: Lecture 6 slide 88\n",
            "answer: High-quality pre-training data is natural, clean, informative, and diverse, and can be found in sources such as\n",
            "books, Wikipedia, news, and scientific papers.\n",
            "\n",
            "question: Can you explain what tokenization means?\n",
            "context: Lecture 3 slide 28\n",
            "answer: Tokenization is the process of splitting large chunks of text into smaller pieces called tokens.\n",
            "\n",
            "question: Why do LLMs need to use tools, and what do they use them for?\n",
            "context: Lecture 10 slide 32\n",
            "answer: LLMs use tools to aid with tasks beyond their ability. They mainly use tools for knowledge, symbolic, and\n",
            "external environment operations.\n",
            "\n",
            "question: How much of my grade is based on participation?\n",
            "context: Lecture 1 slide 15\n",
            "answer: 10% of your grade is determined by course participation/attendance, plus 3% extra credit for attending\n",
            "discussions.\n",
            "\n",
            "question: Define ReAct prompting.\n",
            "context: Lecture 10 slide 17\n",
            "answer: ReAct prompting enables language models to generate reasoning traces and task-specific actions in an interleaved\n",
            "manner, improving interpretability and mitigating errors.\n",
            "\n",
            "question: What are content overlap metrics, and what are their strengths and weaknesses?\n",
            "context: Lecture 13 slides 78-80\n",
            "answer: Content overlap metrics involve computing a similarity score between generated and gold-standard text. They are\n",
            "fast and efficient to calculate, but are not ideal for open-ended tasks and they have no concept of semantic\n",
            "relatedness.\n",
            "\n",
            "question: What is an n-gram model?\n",
            "context: Lecture 2 slide 43\n",
            "answer: An n-gram model is a language model that determines the probability of a word from the previous n-1 words.\n",
            "\n",
            "question: What are the problems with human evaluations?\n",
            "context: Lecture 14 slide 20\n",
            "answer: Human evaluations are slow, expensive, inconsistent, irreproducible, and sometimes illogical, with risks of\n",
            "misinterpretation and potential reliance on LLMs by crowd workers.\n",
            "\n",
            "question: What is layer normalization, and how does it work?\n",
            "context: Lecture 6 slide 71\n",
            "answer: Layer normalization is a trick to help models learn faster by normalizing to unit mean and standard deviation in\n",
            "each layer, then modulating by learned parameters.\n",
            "\n",
            "question: Tell me the pros and cons of one-hot encoding.\n",
            "context: Lecture 4 slide 36\n",
            "answer: One-hot encoding creates sparse representations without requiring learning, but the feature space must be the\n",
            "same size as the vocabulary and there is no way to encode shared representations of words.\n",
            "\n",
            "question: What is prompt-tuning?\n",
            "context: Lecture 8 slide 46\n",
            "answer: Prompt-tuning is contemporaneous work to pre-tuning, where a single \"soft prompt\" representation is prepended to\n",
            "the embedded input on the encoder side.\n",
            "\n",
            "question: What is the time complexity of beam search?\n",
            "context: Lecture 12 slide 21\n",
            "answer: The time complexity of beam search is O(beam width * vocab size * generation length).\n",
            "\n",
            "question: What are the advantages and applications of persona-based prompting?\n",
            "context: Lecture 11 slide 11\n",
            "answer: Persona-based prompting increases engagement and provides context-aware responses, with applications in\n",
            "recommendation systems, customer support, and specialized domains like medicine and law.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
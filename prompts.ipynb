{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RK05Zyth3kM_"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    {\n",
        "        \"question\": \"List the advantages of pre-training and fine-tuning.\",\n",
        "        \"answer\": \"Applying pre-training and fine-tuning leverages underlying information, reduces reliance on task-specific labeled data, initializes model parameters for more generalizable NLP applications, saves training costs, and provides robust representation of language contexts.\",\n",
        "        \"context\": \"Lecture 8 slide 39\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"List measures of fit used to evaluate language models.\",\n",
        "        \"answer\": \"Likelihood, (negative) log likelihood, and perplexity are measures of fit for evaluating LMs.\",\n",
        "        \"context\": \"Lecture 3 slide 15\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are convolutional neural networks?\",\n",
        "        \"answer\": \"CNNs are models that extract spatial and temporal features from images using convolution operations, with pooling layers reducing dimensionality for computational efficiency.\",\n",
        "        \"context\": \"Lecture 16 slide 10\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the attention method?\",\n",
        "        \"answer\": \"The attention method looks at all input tokens at the same time to decide which ones are the most important to decide the next token. This method treats each token's representation as a query to select information from a set of values.\",\n",
        "        \"context\": \"Lecture 5 slides 45-46\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is needed to achieve a more general world model for simulative reasoning?\",\n",
        "        \"answer\": \"A more general world model requires integrating different spaces for simulation/reasoning, generalist language and vision capabilities, and real-time control of the simulation through action inputs.\",\n",
        "        \"context\": \"Guest lecture 1 slide 51\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are the pros and cons of the encoder-decoder approach?\",\n",
        "        \"answer\": \"The encoder-decoder approach balances bidirectional context use and open-text generation, making it effective for multi-task fine-tuning. However, it requires more text wrangling, is harder to train, and is less flexible for natural language generation.\",\n",
        "        \"context\": \"Lecture 8 slide 67\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is add-one estimation?\",\n",
        "        \"answer\": \"Add-one estimation, which is also called Laplace smoothing, smooths probability calculations by adding one to all frequency counts.\",\n",
        "        \"context\": \"Lecture 2 slide 64\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are the traits of high-quality pre-training data, and where can it be found?\",\n",
        "        \"answer\": \"High-quality pre-training data is natural, clean, informative, and diverse, and can be found in sources such as books, Wikipedia, news, and scientific papers.\",\n",
        "        \"context\": \"Lecture 6 slide 88\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Can you explain what tokenization means?\",\n",
        "        \"answer\": \"Tokenization is the process of splitting large chunks of text into smaller pieces called tokens.\",\n",
        "        \"context\": \"Lecture 3 slide 28\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Why do LLMs need to use tools, and what do they use them for?\",\n",
        "        \"answer\": \"LLMs use tools to aid with tasks beyond their ability. They mainly use tools for knowledge, symbolic, and external environment operations.\",\n",
        "        \"context\": \"Lecture 10 slide 32\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How much of my grade is based on participation?\",\n",
        "        \"answer\": \"10% of your grade is determined by course participation/attendance, plus 3% extra credit for attending discussions.\",\n",
        "        \"context\": \"Lecture 1 slide 15\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Define ReAct prompting.\",\n",
        "        \"answer\": \"ReAct prompting enables language models to generate reasoning traces and task-specific actions in an interleaved manner, improving interpretability and mitigating errors.\",\n",
        "        \"context\": \"Lecture 10 slide 17\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are content overlap metrics, and what are their strengths and weaknesses?\",\n",
        "        \"answer\": \"Content overlap metrics involve computing a similarity score between generated and gold-standard text. They are fast and efficient to calculate, but are not ideal for open-ended tasks and they have no concept of semantic relatedness.\",\n",
        "        \"context\": \"Lecture 13 slides 78-80\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is an n-gram model?\",\n",
        "        \"answer\": \"An n-gram model is a language model that determines the probability of a word from the previous n-1 words.\",\n",
        "        \"context\": \"Lecture 2 slide 43\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are the problems with human evaluations?\",\n",
        "        \"answer\": \"Human evaluations are slow, expensive, inconsistent, irreproducible, and sometimes illogical, with risks of misinterpretation and potential reliance on LLMs by crowd workers.\",\n",
        "        \"context\": \"Lecture 14 slide 20\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is layer normalization, and how does it work?\",\n",
        "        \"answer\": \"Layer normalization is a trick to help models learn faster by normalizing to unit mean and standard deviation in each layer, then modulating by learned parameters.\",\n",
        "        \"context\": \"Lecture 6 slide 71\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Tell me the pros and cons of one-hot encoding.\",\n",
        "        \"answer\": \"One-hot encoding creates sparse representations without requiring learning, but the feature space must be the same size as the vocabulary and there is no way to encode shared representations of words.\",\n",
        "        \"context\": \"Lecture 4 slide 36\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is prompt-tuning?\",\n",
        "        \"answer\": \"Prompt-tuning is contemporaneous work to pre-tuning, where a single \\\"soft prompt\\\" representation is prepended to the embedded input on the encoder side.\",\n",
        "        \"context\": \"Lecture 8 slide 46\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the time complexity of beam search?\",\n",
        "        \"answer\": \"The time complexity of beam search is O(beam width * vocab size * generation length).\",\n",
        "        \"context\": \"Lecture 12 slide 21\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are the advantages and applications of persona-based prompting?\",\n",
        "        \"answer\": \"Persona-based prompting increases engagement and provides context-aware responses, with applications in recommendation systems, customer support, and specialized domains like medicine and law.\",\n",
        "        \"context\": \"Lecture 11 slide 11\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "for prompt in prompts:\n",
        "    for key, value in prompt.items():\n",
        "        print(textwrap.fill(f\"{key}: {value}\", 120))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXQL_0Le5wBG",
        "outputId": "1d34d45f-7890-436d-c6ca-919518eb5678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question: List the advantages of pre-training and fine-tuning.\n",
            "answer: Applying pre-training and fine-tuning leverages underlying information, reduces reliance on task-specific\n",
            "labeled data, initializes model parameters for more generalizable NLP applications, saves training costs, and provides\n",
            "robust representation of language contexts.\n",
            "context: Lecture 8 slide 39\n",
            "\n",
            "question: List measures of fit used to evaluate language models.\n",
            "answer: Likelihood, (negative) log likelihood, and perplexity are measures of fit for evaluating LMs.\n",
            "context: Lecture 3 slide 15\n",
            "\n",
            "question: What are convolutional neural networks?\n",
            "answer: CNNs are models that extract spatial and temporal features from images using convolution operations, with\n",
            "pooling layers reducing dimensionality for computational efficiency.\n",
            "context: Lecture 16 slide 10\n",
            "\n",
            "question: What is the attention method?\n",
            "answer: The attention method looks at all input tokens at the same time to decide which ones are the most important to\n",
            "decide the next token. This method treats each token's representation as a query to select information from a set of\n",
            "values.\n",
            "context: Lecture 5 slides 45-46\n",
            "\n",
            "question: What is needed to achieve a more general world model for simulative reasoning?\n",
            "answer: A more general world model requires integrating different spaces for simulation/reasoning, generalist language\n",
            "and vision capabilities, and real-time control of the simulation through action inputs.\n",
            "context: Guest lecture 1 slide 51\n",
            "\n",
            "question: What are the pros and cons of the encoder-decoder approach?\n",
            "answer: The encoder-decoder approach balances bidirectional context use and open-text generation, making it effective\n",
            "for multi-task fine-tuning. However, it requires more text wrangling, is harder to train, and is less flexible for\n",
            "natural language generation.\n",
            "context: Lecture 8 slide 67\n",
            "\n",
            "question: What is add-one estimation?\n",
            "answer: Add-one estimation, which is also called Laplace smoothing, smooths probability calculations by adding one to\n",
            "all frequency counts.\n",
            "context: Lecture 2 slide 64\n",
            "\n",
            "question: What are the traits of high-quality pre-training data, and where can it be found?\n",
            "answer: High-quality pre-training data is natural, clean, informative, and diverse, and can be found in sources such as\n",
            "books, Wikipedia, news, and scientific papers.\n",
            "context: Lecture 6 slide 88\n",
            "\n",
            "question: Can you explain what tokenization means?\n",
            "answer: Tokenization is the process of splitting large chunks of text into smaller pieces called tokens.\n",
            "context: Lecture 3 slide 28\n",
            "\n",
            "question: Why do LLMs need to use tools, and what do they use them for?\n",
            "answer: LLMs use tools to aid with tasks beyond their ability. They mainly use tools for knowledge, symbolic, and\n",
            "external environment operations.\n",
            "context: Lecture 10 slide 32\n",
            "\n",
            "question: How much of my grade is based on participation?\n",
            "answer: 10% of your grade is determined by course participation/attendance, plus 3% extra credit for attending\n",
            "discussions.\n",
            "context: Lecture 1 slide 15\n",
            "\n",
            "question: Define ReAct prompting.\n",
            "answer: ReAct prompting enables language models to generate reasoning traces and task-specific actions in an interleaved\n",
            "manner, improving interpretability and mitigating errors.\n",
            "context: Lecture 10 slide 17\n",
            "\n",
            "question: What are content overlap metrics, and what are their strengths and weaknesses?\n",
            "answer: Content overlap metrics involve computing a similarity score between generated and gold-standard text. They are\n",
            "fast and efficient to calculate, but are not ideal for open-ended tasks and they have no concept of semantic\n",
            "relatedness.\n",
            "context: Lecture 13 slides 78-80\n",
            "\n",
            "question: What is an n-gram model?\n",
            "answer: An n-gram model is a language model that determines the probability of a word from the previous n-1 words.\n",
            "context: Lecture 2 slide 43\n",
            "\n",
            "question: What are the problems with human evaluations?\n",
            "answer: Human evaluations are slow, expensive, inconsistent, irreproducible, and sometimes illogical, with risks of\n",
            "misinterpretation and potential reliance on LLMs by crowd workers.\n",
            "context: Lecture 14 slide 20\n",
            "\n",
            "question: What is layer normalization, and how does it work?\n",
            "answer: Layer normalization is a trick to help models learn faster by normalizing to unit mean and standard deviation in\n",
            "each layer, then modulating by learned parameters.\n",
            "context: Lecture 6 slide 71\n",
            "\n",
            "question: Tell me the pros and cons of one-hot encoding.\n",
            "answer: One-hot encoding creates sparse representations without requiring learning, but the feature space must be the\n",
            "same size as the vocabulary and there is no way to encode shared representations of words.\n",
            "context: Lecture 4 slide 36\n",
            "\n",
            "question: What is prompt-tuning?\n",
            "answer: Prompt-tuning is contemporaneous work to pre-tuning, where a single \"soft prompt\" representation is prepended to\n",
            "the embedded input on the encoder side.\n",
            "context: Lecture 8 slide 46\n",
            "\n",
            "question: What is the time complexity of beam search?\n",
            "answer: The time complexity of beam search is O(beam width * vocab size * generation length).\n",
            "context: Lecture 12 slide 21\n",
            "\n",
            "question: What are the advantages and applications of persona-based prompting?\n",
            "answer: Persona-based prompting increases engagement and provides context-aware responses, with applications in\n",
            "recommendation systems, customer support, and specialized domains like medicine and law.\n",
            "context: Lecture 11 slide 11\n",
            "\n"
          ]
        }
      ]
    }
  ]
}

CSE 156 Natural Language Processing 
9 - Pretraining and prompting
Instructor: Lianhui Qin 
Slides adapted from Yejin Choi, Graham Neubig and Joyce Chai 
1 
Recap 
CSE 156 NLP 2 Pretraining
Overview: The Paradigm Shift Training Set (Dev) 
Classic Deep Learning 
Train Development Set  
slow 
(Dev) 
Validation Set (Val) Validate 
Test Set (Test) Test 
Pre-training Set (Pre) 
Development Set  (Dev) 
Test Set (Test) 
Pre-train 
slow 
Fine 
Test fast Tune 
Since 2018 (Elmo) 
CSE 156 NLP 3 Pretraining
Self-supervised Pre-training for Learning Underlying  Patterns, Structures, and Semantic Knowledge 
• Pre-training through language modeling [Dai  
and Le, 2015] 
• Model , the probability  
Pθ(wt|w1:t−1) 
distribution of the next word given previous  contexts. 
• There’s lots of (English) data for this! E.g.,  books, websites. 
• Self-supervised training of a neural  network to perform the language modeling  task with massive raw text data. 
• Save the network parameters to reuse later. 
are composed of tiny water droplet EOS 
Decoder 
(Transformers, LSTM, …) 
Clouds are composed of tiny water droplet 
CSE 156 NLP 4 Pretraining
Pre-training Data 
• Ideally, we want high-quality data for pre-training. 
• High-quality: natural, clean, informative, and diverse; 
• Books, Wikipedia, news, scientific papers; 
• High quality data eventually runs out.  
• In practice, internet is the most viable option for data. 
• In the digital era, the web is the go-to place for general domain human knowledge; • It is massive and unlikely to grow slower than computing resources* 
• Publicly available* 
* Are these claim still true these days? Questionable. 
Lecture 10
CSE 156 NLP 5 Pretraining 
Pre-training Data 
• Web data is plentiful, but can be challenging to work with. • What counts as content? 
• Ads, recommendation, navigation, etc. 
• Multimedia: images, videos, tables, etc. 
• HTML? What if you want to train a code language model? 
• Spam? 
• Copyright and usage constraints can get extremely complicated • Data is contaminated with auto-generated text 
• Not just from LLM usage, but also tons of templated text. 
• Training on synthetic data can lead to language model collapse (Seddik et al 2024). 
Seddik et al. "How bad is training on synthetic data? a statistical analysis of language model collapse." arXiv preprint arXiv:2404.05090  (2024).
CSE 156 NLP 6 Pretraining 
Pre-training Data 
• How to harvest a large number of seed URLs efficiently? • How to select “high quality” URLs and skip over “bad” URLs • Some cases are clear cut: spammy, unsafe, NSFW, etc. • Some are hard to detect or up to debate: toxic and biased content • How to keep the crawl up-to-date  
• Given a fixed compute budget each month, is it better to crawl new  webpages, or recrawl old ones that might’ve changed? 
Lecture 10
CSE595 - Natural Language Processing - Fall 2024 
CSE 156 NLP 7 Pretraining 
Pre-training Data 
• The diversity of pre-training data matters 
Zhao et al. "A survey of large language models." arXiv preprint arXiv:2303.18223 (2023).
Lecture 10 
CSE 156 NLP 8 Pretraining 
Overview: The Paradigm Shift Training Set (Dev) 
Classic Deep Learning 
Train Development Set  
slow 
(Dev) 
Validation Set (Val) Validate 
Test Set (Test) Test 
Pre-training Set (Pre) 
Development Set  (Dev) 
Test Set (Test) 
slowSince 2018 (Elmo)
Pre-train 
Fine 
Test fast 
Tune 
Pre-training Set (Pre) Test Set (Test) 
Lecture 10 
Pre-train 
slow 
In-Context Learning (Zero-shot Learning) 
Since 2020 (GPT-3) 
Test 
CSE 156 NLP 9 Pretraining 
Supervised Fine-tuning for Specific Tasks 
Step 1: 
Pre-training 
are composed of tiny water droplet EOS 
Decoder 
(Transformers, LSTM, …) 
Clouds are composed of tiny water droplet Abundant data; learn general language 
Step 2: 
Fine-tuning 
or 
Decoder 
(Transformers, LSTM, …) 
… the movie was … 
Limited data; adapt to the task 
CSE 156 NLP 
10 Pretraining
Parameter-Efficient Fine-tuning 
Instead of updating all parameters in the massive neural network (up to many  billions of parameters), can we make fine-tuning more efficient? 
or Decoder 
or Decoder 
(Transformers, LSTM, …) 
… the movie was … 
Full Fine-tuning 
Updating all parameters  
CSE 156 NLP 
(Transformers, LSTM, …) 
… the movie was … 
Parameter-Efficient Fine-tuning 
Updating a few existing or new parameters 11 Pretraining
Adapter 
[Houlsby, 2019] 
Updated
• Injecting new layers (randomly initialized) into the original network,  keeping other parameters frozen 
CSE 156 NLP 12 Pretraining 
Prefix-tuning [Li and Liang, 2021] 
• Learning a small continuous  task-specific vector (called  
the prefix) to each  
Frozen
transformer block, while  keeping the pre-trained LM  frozen 
• With 0.1% parameter is  comparable to full fine 
tuning, especially under low data regime 
Updated 
CSE 156 NLP 13 Pretraining 
Prompt-tuning [Lester et al., 2021] • Contemporaneous work to  
prefix-tuning 
• A single “soft prompt”  representation that is  
prepended to the  
embedded input on the  encoder side 
• Require fewer parameters  than prefix-tuning 
Updated 
Frozen
CSE 156 NLP 14 Pretraining 
Low-Rank Adaptation (LoRA) • Main Idea: learn a low-rank “diff”  
between the pre-trained and fine-tuned  weight matrices. 
• ~10,000x less fine-tuned parameters,  ~3x GPU memory requirement. 
• On-par or better than fine-tuning all  model parameters in model quality on  RoBERTa, DeBERTa, GPT-2, and GPT-3. • Easier to learn than prefix-tuning. 
[Hu et al., 2021] 
B ∈ ℝd×r 
A ∈ ℝr×d
where rank r ≪ min(d, d) Frozen Updated 
W0 + ΔW = W0 + BA 
CSE 156 NLP 15 Pretraining 
3 Pre-training Paradigms/Architectures• E.g., BERT, RoBERTa, DeBERTa, … 
Encoder • 
• Autoencoder model • Masked language modeling 
Encoder-Decoder• E.g., T5, BART, … • seq2seq model 
• E.g., GPT, GPT2, GPT3, … 
Decoder 
• Autoregressive model 
• Left-to-right language modeling 
CSE 156 NLP 16 Pretraining 
3 Pre-training Paradigms/Architectures 
Encoder • Bidirectional; can condition  on the future context 
• 
Encoder-Decoder• Map two sequences of  different length together
Decoder• Language modeling; can only  condition on the past context 
CSE 156 NLP 17 Pretraining 
Encoder: Training Objective 
[Devlin et al., 2018] 
• How to encode information from both bidirectional contexts? • General Idea: text reconstruction! 
• Your time is [MASK], so don't [MASK] it living someone else's life.  
 limited waste  
 dogma living  
Don't be trapped by [MASK], which is [MASK] with the results of  other [MASK]'s thinking. — [MASK] Jobs 
people Steve  
went store, 
h1, …, h 
I [M] to the [M] 
h1, …, h = Encoder( 1,…, ) 
∼ + 
~ 
Only add loss terms from the masked tokens. If is the masked version  ( |~)
of , we’re learning . Called Masked Language model (MLM). 
CSE 156 NLP 18 Pretraining 
Bidirectional Encoder Encoder: BERT Representations from Transformers [Devlin et al., 2018]
• 2 Pre-training Objectives: 
• Masked LM: Choose a random 15% of tokens to  predict. 
[Predict these!] to went 
store 
• For each chosen token: 
WHY keeping some tokens unchanged? There’s no [MASK] during fine-tuning time! 
• Replace it with [MASK] 80% of the time. 
• Replace it with a random token 10% of the time. • Leave it unchanged 10% of the time (but still  predict it!). 
• Next Sentence Prediction (NSP) 
• 50% of the time two adjacent sentences are in the  correct order. 
• This actually hurts model learning based on later  work! 
Encoder 
I pizza to the [M] 
[Replaced] [Not replaced] [Masked] 
CSE 156 NLP 19 Pretraining 
Bidirectional Encoder Encoder: BERT Representations from Transformers [Devlin et al., 2018]
• 2 Pre-training Objectives: 
• Masked LM: Choose a random 15% of tokens to  predict. 
[Predict these!] to went 
store 
• For each chosen token: 
WHY keeping some tokens unchanged? There’s no [MASK] during fine-tuning time! 
• Replace it with [MASK] 80% of the time. 
• Replace it with a random token 10% of the time. • Leave it unchanged 10% of the time (but still  predict it!). 
• Next Sentence Prediction (NSP) 
• 50% of the time two adjacent sentences are in the  correct order. 
• This actually hurts model learning based on later  work! 
Encoder 
I pizza to the [M] 
[Replaced] [Not replaced] [Masked] 
CSE 156 NLP 20 Pretraining 
Bidirectional Encoder Encoder: BERT Representations from Transformers [Devlin et al., 2018]
• 2 Pre-training Objectives: 
• Masked LM: Choose a random 15% of tokens to  predict. 
• For each chosen token: 
• Replace it with [MASK] 80% of the time. 
• Replace it with a random token 10% of the time. • Leave it unchanged 10% of the time (but still  
[Predict these!] to went 
Encoder 
store 
predict it!). 
• Next Sentence Prediction (NSP) 
• 50% of the time two adjacent sentences are in the  correct order. 
• This actually hurts model learning based on later  work! 
I pizza to the [M] 
[Replaced] [Not replaced] [Masked] 
CSE 156 NLP 21 Pretraining 
Bidirectional Encoder Encoder: BERT Representations from Transformers [Devlin et al., 2018] 
• 2 Pre-training Objectives: Special token added to the  
Special token to  
Final embedding is the sum of  
• Masked LM: Choose a random 15% of tokens to predict. 
beginning of each input sequence • For each chosen token: 
separate sentence A/B 
all three!
• Replace it with [MASK] 80% of the time. 
• Replace it with a random token 10% of the time. 
• Leave it unchanged 10% of the time (but still predict it!). • Next Sentence Prediction (NSP) 
• 50% of the time two adjacent sentences are in the correct order. • This actually hurts model learning based on later work! 
Learned embedding to every token indicating  
whether it belongs to sentence A or sentence B Position of the token in the entire sequence 
CSE 156 NLP 22 Pretraining 
Bidirectional Encoder Encoder: BERT Representations from Transformers [Devlin et al., 2018] 
• SOTA at the time on a wide range of tasks after fine-tuning! • QQP: Quora Question Pairs (detect paraphrase questions) 
• QNLI: natural language inference over question answering data 
• SST-2: sentiment analysis 
• CoLA: corpus of linguistic acceptability (detect whether sentences are grammatical.) • STS-B: semantic textual similarity 
• MRPC: microsoft paraphrase corpus 
• RTE: a small natural language inference corpus
CSE 156 NLP 23 Pretraining 
Bidirectional Encoder Encoder: BERT Representations from Transformers [Devlin et al., 2018] 
• Two Sizes of Models 
SWAG 
• Base: 110M, 4 Cloud TPUs, 4 days • Large: 340M, 16 Cloud TPUs, 4 days • Both models can be fine-tuned with  single GPU 
• The larger the better! 
• MLM converges slower than Left-to Right at the beginning, but out performers it eventually
CSE 156 NLP 24 Pretraining 
Encoder: RoBERTa 
[Liu et al., 2019] 
• Original BERT is significantly undertrained! 
• More data (16G => 160G) • Pre-train for longer 
• Bigger batches 
All around better than BERT!
• Removing the next sentence prediction (NSP) objective 
• Training on longer sequences 
• Dynamic masking, randomly masking out different tokens 
• A larger byte-level BPE vocabulary containing 50K sub-word units CSE 156 NLP 25 Pretraining 
Encoder: Other Variations of BERT 
• ALBERT [Lan et al., 2020]: incorporates two parameter reduction techniques that lift  the major obstacles in scaling pre-trained models  
• DeBERTa [He et al., 2021]: decoding-enhanced BERT with disentangled attention  • SpanBERT [Joshi et al., 2019]: masking contiguous spans of words makes a harder,  more useful pre-training task 
• ELECTRA [Clark et al., 2020]: corrupts texts by replacing some tokens with  plausible alternatives sampled from a small generator network, then train a discriminative  model that predicts whether each token in the corrupted input was replaced by a generator  sample or not. 
• DistilBERT [Sanh et al., 2019]: distilled version of BERT that’s 40% smaller • TinyBERT [Jiao et al., 2019]: distill BERT for both pre-training & fine-tuning • … 
CSE 156 NLP 26 Pretraining
Encoder: Pros & Cons 
• Consider both left and right context 
• Capture intricate contextual relationships 
• Not good at generating open-text from left-to right, one token at a time 
make/brew/craft 
Encoder 
Iroh goes to [M] tasty tea 
goes to make tasty tea END Decoder
Iroh goes to make tasty tea 
CSE 156 NLP 27 Pretraining 
3 Pre-training Paradigms/Architectures 
Encoder • Bidirectional; can condition  on the future context 
• 
Encoder-Decoder• Map two sequences of  different length together
Decoder• Language modeling; can only  condition on the past context 
CSE 156 NLP 28 Pretraining 
Encoder-Decoder: Architecture 
• Moving towards open-text  
generation… 
• Encoder builds a representation of  the source and gives it to the decoder • Decoder uses the source  
representation to generate the target  sentence 
wt1+2, . . . 
wt1+1, . . . ,wt2 
w1, . . . ,wt1 
• The encoder portion benefits from  bidirectional context; the decoder portion is used to train the whole  model through language modeling 
h1, . . . , ht1= Encoder(w1, . . . ,wt1) 
ht1+1, . . . , ht2= Decoder(wt1+1, . . . ,wt2, h1, . . . , ht1) yi ∼ Ahi + b, i > t 
[Raffel et al., 2018]
CSE 156 NLP 29 Pretraining 
Encoder-Decoder: An Machine Translation Example 
• Moving towards open-text  
generation… 
• Encoder builds a representation of  
the source and gives it to the decoder 
• Decoder uses the source  
representation to generate the target  
sentence 
• The encoder portion benefits from  
bidirectional context; the decoder 
portion is used to train the whole  
model through language modeling 
[Lena Viota Blog]
CSE 156 NLP 30 Pretraining 
Encoder-Decoder: Training Objective 
• T5 [Raffel et al., 2018] 
• Text span corruption (denoising): Replace  
different-length spans from the input with  
unique placeholders (e.g., <extra_id_0>);  
decode out the masked spans. 
• Done during text preprocessing: training  
uses language modeling objective at the  
decoder side 
CSE 156 NLP 31 Pretraining
Encoder-Decoder: T5 [Raffel et al., 2018]
• Encoder-decoders works better than decoders 
• Span corruption (denoising) objective works better than language modeling CSE 156 NLP 32 Pretraining 
Encoder-Decoder: T5 [Raffel et al., 2018] 
• Text-to-Text: convert NLP tasks into input/output text sequences • Dataset: Colossal Clean Crawled Corpus (C4), 750G text data! • Various Sized Models: 
• Base (222M) 
• Small (60M) 
• Large (770M) 
• 3B 
• 11B 
• Achieved SOTA with scaling & purity of data 
[Google Blog]
CSE 156 NLP 33 Pretraining 
Encoder-Decoder: Pros & Cons 
• A nice middle ground between leveraging bidirectional contexts and open-text generation 
• Good for multi-task fine-tuning 
• Require more text wrangling 
• Harder to train 
• Less flexible for natural language generation
CSE 156 NLP 34 Pretraining 
3 Pre-training Paradigms/Architectures 
Encoder • Bidirectional; can condition  on the future context 
• 
Encoder-Decoder• Map two sequences of  different length together
Decoder• Language modeling; can only  condition on the past context 
CSE 156 NLP 35 Pretraining 
Decoder: Training Objective • Many most famous generative LLMs are decoder 
only 
• e.g., GPT1/2/3/4, Llama1/2 
• Language modeling! Natural to be used for open text generation 
• Conditional LM:  
p(wt|w1, . . . ,wt−1, x) 
• Conditioned on a source context to generate  
x 
from left-to-right 
• Can be fine-tuned for natural language generation  (NLG) tasks, e.g., dialogue, summarization. 
w2,w3,w4,w5,w6 w1,w2,w3,w4,w5 
A, b
h1, . . . , h5 
CSE 156 NLP 36 Pretraining 
Decoder: Training Objective • Customizing the pre-trained model for  
downstream tasks: 
• Add a linear layer on top of the last  hidden layer to make it a classifier! • During fine-tuning, trained the randomly  initialized linear layer, along with all  parameters in the neural net. 
or 
Linear 
A, b 
h1, . . . , h5 
Is Santa Claus real figure?
CSE 156 NLP 37 Pretraining 
Decoder: GPT Generative Pre-trained Transformer [Radford et al., 2018]
CSE 156 NLP 38 Pretraining 
How to pick a proper architecture for a given task? 
• Right now decoder-only models seem to dominant the field at the  moment 
• e.g., GPT1/2/3/4, Mistral, Llama1/2 
• T5 (seq2seq) works well with multi-tasking 
• Picking the best model architecture remains an open research  question! 
CSE 156 NLP 39 Pretraining
CSE 156 NLP 
Prompting 
40 Pretraining
Overview: The Paradigm Shift Training Set (Dev) 
Classic Deep Learning 
Train Development Set  
slow 
(Dev) 
Validation Set (Val) Validate 
Test Set (Test) Test 
Pre-training Set (Pre) 
Development Set  (Dev) 
Test Set (Test) 
Pre-training Set (Pre) 
Pre-train 
slow 
Pre-train 
slow 
Fine 
Test fast Tune 
Since 2018 (Elmo) Since 2020 (GPT-3) 
Test Set (Test) 
In-Context Learning (Zero-shot Learning) 
Test 
CSE 156 NLP 41 Lecture 9: Prompting+Chemistry_Reasoning
What is Prompting？ 
□ Encouraging a pre-trained model to make particular predictions by  providing a textual “prompt" specifying the task to be done.
42 
CSE 156 NLP Lecture 9: Prompting 
Basic Prompting (Radford et al. 2018) 
■ Complete a sentence 
x = When a dog sees a squirrel, it will usually 
(GPT-2 Small) 
be afraid of anything unusual. As an exception, that's when a squirrel is usually afraid to bite. 
(GPT-2 XL) lick the squirrel. It will also touch its nose to the squirrel on the tail  and nose if it can.
CSE 156 NLP 43 Lecture 9: Prompting+Chemistry_Reasoning 
Standard Prompting Workflow 
• Fill a prompt template and Predict the answer • Post-process the answer
CSE 156 NLP Lecture 9: Prompting 44 
Prompt Templates 
■ A template where you fill in with an actual input 
Input: x = “I love this movie” 

Template: [x] Overall, it was [z] 

Prompting: x’ = “I love this movie. Overall it was [z]” 
45
CSE 156 NLP Lecture 9: Prompting 
Chat Prompts 
Recently, many models are trained as chatbots 
■ 
■ 
Usually inputs are specified in OpenAI messages format 
messages=[ 
{ 
"role": “system", 
"content": “Please classify movie reviews as 'positive' or ‘negative'." }, 
{ 
"role": “user", 
"content": "This movie is a banger.” 
}, 
] 
■ Roles: 
“system”: message provided to the system to influence behavior ■ 
“user”: message input by the user 
■ 
■ “assistant”: message output by the system
7 
CSE 156 NLP Lecture 9: Prompting 
Chat Prompts Behind the Scenes 
■ Behind the scenes, messages are converted to token strings LLaMa Alpaca
[INST] 
Sys. 
<<SYS>> 
You are an assistant that … <</SYS>>  
[/INST] 
Sys. User 
### Instruction: 
You are an assistant that … 
### Instruction: This  movie is great. 
User [INST]This movie is great.[/INST] 
### Response:  
Asst. 
Positive. 
Asst. 
Positive. 
■ Software: See LiteLLM Prompt Templates 
47 
CSE 156 NLP Lecture 9: Prompting 
Answer Prediction 
■ Given a prompt, predict the answer 
Prompting: x’ = “I love this movie. Overall it was [z]”

Predicting: x’ = “I love this movie. Overall it was fantastic” 
■ 
48 
Use any inference algorithms, as in generation class 
CSE 156 NLP Lecture 9: Prompting 
Output Formatting 
■ For user-facing applications, format in a pretty way 
Markdown Rendering Code 

49
CSE 156 NLP Lecture 9: Prompting 
Post-processing: Output Selection ■ From a longer response, select the information indicative of an answer 
Predicting: x’ = “I love this movie. Overall it was a movie that was simply fantastic”

Extraction: fantastic 
• Classification: identify keywords  
• Regression/numerical problems: identify numbers  
• Code: pull out code snippets in triple-backticks 
50 
CSE 156 NLP Lecture 9: Prompting 
Output Mapping 
■ Given an answer, map it into a class label or continuous value Extraction: fantastic 

Mapping: fantastic => Positive 
■ Often map many extracted words onto a single class 
Positive Negative 
Interesting  Fantastic  Happy 
Boring  
1-star 
…
13 
CSE 156 NLP Lecture 9: Prompting 
Few-shot Prompting (Brown+ 2021) 
■ Provide a few examples of the task together with the instruction 
Instruction Please classify movie reviews as 'positive' or ‘negative’. 
Input: I really don’t like this movie.  
Output: negative 
Examples 
Input: This movie is great! 
Output: positive 
Language Models are Few-Shot Learners, Brown et al. 2020)
52 
CSE 156 NLP Lecture 9: Prompting 
Few-shot Prompting w/ Chat Prompts(OpenAI Cookbook) 
■ For OpenAI models, add “role”: “system” and a “name”: “example_assistant” etc. 
messages=[ 
{ 
"role": “system", 
"content": "You are an assistant that translates corporate jargon into plain English.” }, 
{ 
“role”: “system”,  
“name":"example_user", 
"content": "New synergies will help drive top-line growth.” 
}, 
{ 
"role": “system", 
"name": “example_assistant", 
"content": "Things working well together will increase revenue.” 
}, 
..., 
{ 
"role": “user", 
"content": "This late pivot means we don't have time to boil the ocean for the client deliverable.” }, 
] 
53
CSE 156 NLP Lecture 9: Prompting 
LMs are Sensitive to Small Changes in In-context Examples ■ Example ordering (Lu et al. 2021) ■ Label balance (Zhang et al. 2022) 
■ Label coverage (Zhang et al. 2022) 
54
CSE 156 NLP Lecture 9: Prompting 
But Effects are Sometimes Counter-intuitive(Min et al. 2022) 
■ Replacing correct labels with random labels sometimes barely hurts accuracy ■ More demonstrations can sometimes hurt accuracy 

55
CSE 156 NLP Lecture 9: Prompting 
Chain of Thought Prompting(Wei et al. 2022) ■ Get the model to explain its reasoning before making an answer 
■ Provides the model with adaptive computation time 
56
CSE 156 NLP Lecture 9: Prompting 
Unsupervised Chain-of-thought Prompting (Kojima et al. 2022) 
■ Just adding a prompt that encourages the model to explain decisions can induce reasoning 

■Note: GPT models reason even w/o specific instructions now (probably due to instruction tuning)
21 
CSE 156 NLP Lecture 9: Prompting 
Structuring Outputs as Programs can Help(Madaan et al. 2022) 
• When predicting a structured  
output, using a programming  
language instead of natural  
language often increases accuracy 
• Why? Programs are highly 
structured and included in pre 
training data 
• Asking the model to generate  
JSON can help formatting problems 
58
CSE 156 NLP Lecture 9: Prompting 
Program-aided Language Models(Gao et al. 2022) 
• Using a program to generate 
outputs can be more precise 
than asking the LM to do so 
• Especially useful for numeric 
questions 
• See ChatGPT code interpreter, 
Bard code execution 
• (More on agents/tools later) 
59
CSE 156 NLP Lecture 9: Prompting 
Prompt Engineering
CSE 156 NLP Lecture 9: Prompting 60 
Design of Prompts 
■ Manual 
□ Configure a manual template based on the characteristics of the task ■ Automated search 
□ Search in discrete space 
□ Search in continuous space
CSE 156 NLP Lecture 9: Prompting 61 
Manual Engineering: Format 
■ Make sure that the format matches that of a trained model (e.g. chat format) 
This can have a large effect on models! (Sclar et al. 2023) ■ 

62
CSE 156 NLP Lecture 9: Prompting 
Manual Engineering: Instructions 
Instructions should be clear, concise and easy to understand Good 
■ 
examples: https://www.promptingguide.ai/introduction/tips ■ 
• Less Precise: 
• Explain the concept prompt engineering. Keep the explanation short, only a few sentences, and don't be too descriptive. 
• More Precise: 
• Use 2-3 sentences to explain the concept of prompt engineering to a high school student.
Similar to humans, but (right now) LMs don’t complain when you’re vague 
CSE 156 NLP Lecture 9: Prompting 63 
Methods for Automatic Prompt Engineering 
• Prompt paraphrasing 
• Gradient-based discrete prompt search Prompt tuning • Prefix tuning
CSE 156 NLP Lecture 9: Prompting 64 
Prompt Paraphrasing 
• Paraphrase an existing prompt to get other candidates (Jiang et al. 2019) 
[X] shares a border with [Y]. 
Paraphrasing Model
[X] has a common border with [Y]. [X] adjoins [Y]. 
…… 
•Can be done through iterative paraphrasing (Zhou et al. 2021) 30 
CSE 156 NLP Lecture 9: Prompting 
Gradient-based Search (Shin et al. 2020) 
• Automatically optimize arbitrary prompts based on existing words 66
CSE 156 NLP Lecture 9: Prompting 
CSE 156 NLP 
Reasoning? 
67 Pretraining
StructChem:  

Structured Chemistry Reasoning with Large Language  Models 
Siru Ouyang Zhuosheng  
Zhang Bing Yan Xuan Liu Yejin Choi Jiawei Han Lianhui Qin 
ICML2024
Previous chemistry tasks

Previous chemistry tasks 
Direct question-answering;  
Lack complex reasoning process 


Unifying textual and molecule representation via multi-tasking.
Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. Fang et al., arXiv 2306. 


How will chemicals  
behave at equilibrium? 
Kc 
 means the equilibrium  
constant. 
Understanding of this certain  
chemistry reaction. 
SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models Wang et al., arXiv 2023.

For the given reaction: 
N2(g) + O2(g) → N2O(g) 
The equilibrium constant K_c is defined as follows: Kc=[N2O] ([N2] × [O2]) 
[N2] [O2] [N2O] 
where , , and are the molar  N2 O2 N2O 
concentrations of , , and respectively at  equilibrium. 
[N2] =2.80 × 10−4mol 
2.00L= 1.40 × 10−4M 
Kc 
Then plug these values into the formula for : Kc=(1.00 × 10−2) 
1.40 × 10−4 × 1.25 × 10−5= 5.71 × 106K

For the given reaction: 
N2(g) + O2(g) → N2O(g) 
The equilibrium constant K_c is defined as follows: Kc=[N2O] ([N2] × [O2]) 
[N2] [O2] [N2O] 
where , , and are the molar  N2 O2 N2O 
concentrations of , , and respectively at  equilibrium. 
[N2] =2.80 × 10−4mol 
2.00L= 1.40 × 10−4M 
Kc 
Then plug these values into the formula for : Kc=(1.00 × 10−2) 
1.40 × 10−4 × 1.25 × 10−5= 5.71 × 106K

For the given reaction: 
N2(g) + O2(g) → N2O(g) 
The equilibrium constant K_c is defined as follows: Kc=[N2O] 
([N2] × [O2]) 
[N2] [O2] [N2O] 
where , , and are the molar  N2 O2 N2O 
concentrations of , , and respectively at  equilibrium. 
[N2] =2.80 × 10−4mol 
2.00L= 1.40 × 10−4M 
Kc 
Then plug these values into the formula for : 
Equation balancing 
Correct format of theorem 
2N2(g) + O2(g) → 2N2O(g) 
Kc=[N2O]a 
([N2]b × [O2]c) 
Kc=(1.00 × 10−2) 
1.40 × 10−4 × 1.25 × 10−5= 5.71 × 106K 
Plug-ins
Kc=(1.00 × 10−2)2 
(1.40 × 10−4)2 × 1.25 × 10−5= 4.08 × 108K 

For the given reaction: 
N2(g) + O2(g) → N2O(g) 
The equilibrium constant K_c is defined as follows: 
Firstly, we need to balance the given reaction: 2N2(g) + O2(g) → 2N2O(g) 
The equilibrium constant K_c is defined as  
Kc=[N2O] 
([N2] × [O2]) 
[N2] [O2] [N2O] 
follows: 
Kc=[N2O]a ([N2]b × [O2]c) 
where , , and are the molar  N2 O2 N2O 
concentrations of , , and respectively at  equilibrium. 
[N2] =2.80 × 10−4mol 
2.00L= 1.40 × 10−4M 
Kc 
Then plug these values into the formula for : Kc=(1.00 × 10−2) 
1.40 × 10−4 × 1.25 × 10−5= 5.71 × 106K 
where a,b,c are the coefficients of different  matters and [*] is the molarity of *. 
[N2] =2.80 × 10−4mol 
2.00L= 1.40 × 10−4M 
Kc 
Plugin the values and solving for numerically  we get  
Kc=(1.00 × 10−2)2 
(1.40 × 10−4)2 × 1.25 × 10−5= 4.08 × 108K

For the given reaction: 
Firstly, we need to balance the given reaction: 
N2(g) + O2(g) → N2O(g) 
Accurate domain knowledge 
2N2(g) + O2(g) → 2N2O(g) 
The equilibrium constant K_c is defined as follows: 
The equilibrium constant K_c is defined as  
Kc=[N2O] 
([N2] × [O2]) 
[N2] [O2] [N2O] 
follows: 
Precise scientific reasoningC 
Kc=[N2O]a ([N2]b × [O2]c) 
where , , and are the molar  N2 O2 N2O 
concentrations of , , and respectively at  equilibrium. 
where a,b,c are the coefficients of different  matters and [*] is the molarity of *. [N2] =2.80 × 10−4mol 
[N2] =2.80 × 10−4mol 
Symbolic/structured reasoning 
2.00L= 1.40 × 10−4M 
2.00L= 1.40 × 10−4M 
Kc 
Then plug these values into the formula for : Kc=(1.00 × 10−2) 
1.40 × 10−4 × 1.25 × 10−5= 5.71 × 106K 
Kc 
Plugin the values and solving for numerically  we get  
Kc=(1.00 × 10−2)2 
(1.40 × 10−4)2 × 1.25 × 10−5= 4.08 × 108K 
It’s really challenging task… 

For the given reaction: 
Firstly, we need to balance the given reaction: 
N2(g) + O2(g) → N2O(g) 
Accurate domain knowledge 
2N2(g) + O2(g) → 2N2O(g) 
The equilibrium constant K_c is defined as follows: 
The equilibrium constant K_c is defined as  
Kc=[N2O] 
([N2] × [O2]) 
[N2] [O2] [N2O] 
follows: 
Precise scientific reasoningC 
Kc=[N2O]a ([N2]b × [O2]c) 
where , , and are the molar  N2 O2 N2O 
concentrations of , , and respectively at  equilibrium. 
where a,b,c are the coefficients of different  matters and [*] is the molarity of *. [N2] =2.80 × 10−4mol 
[N2] =2.80 × 10−4mol 
Symbolic/structured reasoning 
2.00L= 1.40 × 10−4M 
2.00L= 1.40 × 10−4M 
Kc 
Plugin the values and solving for numerically  
Kc 
Then plug these values into the formula for : 
we get  
Fail on the task 
Kc=(1.00 × 10−2) 
1.40 × 10−4 × 1.25 × 10−5= 5.71 × 106K 
Undergrad students from top university 
Kc=(1.00 × 10−2)2 
(1.40 × 10−4)2 × 1.25 × 10−5= 4.08 × 108K Score: 27/100 
Recap on related works 
Self-Consistency  
(Wang et al., 2022):  
majority vote 
CoT (Wei et al.,  
2022)Self-Refine (Madaan  et al., 2023):  
feedback machinist 
Decomposed  
Prompting (Khot et  
al., 2022): modular
Recap on related works 
Self-Consistency  
(Wang et al., 2022):  
majority vote 
Task-wise: centered around arithmetic, commonsense, and  
logical reasoning problems. 
CoT (Wei et al.,  
2022)Self-Refine (Madaan  et al., 2023):  
feedback machinist 
Method-wise: Micro-level decomposition varying from sample  
to sample; Sequential prompting; 
Correct answers risk being swayed by feedback.
Decomposed  
Prompting (Khot et  
al., 2022): modular 
StructChem: 
A structural approach that elicits chemistry reasoning in LLMs • Overview 
Why not retrieval? 
Chemistry problem Structured instruction 
F0 R0Formulae generation Reasoning Process 
Iterative Review and Refinement  
with certain strategy 
Formulae generation with  step-by-step reasoning  
Answer 

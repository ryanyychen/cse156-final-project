{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#### pip installs ####\n",
    "!pip install langchain-experimental langchain-huggingface qdrant-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#### Imports ####\n",
    "# Reading Files #\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Embeddings #\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Database #\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### Initializations ####\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using \" + DEVICE)\n",
    "\n",
    "# Text splitter\n",
    "# Use a chunker to chunk text\n",
    "# Different options here: length chunking, character break chunking, semantic chunking\n",
    "# Using semantic chunking for best separation of different information to help retrieval\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "text_splitter = SemanticChunker(embeddings=embeddings)\n",
    "\n",
    "# Dense embedder\n",
    "DENSE_MODEL_NAME = \"intfloat/e5-base\"\n",
    "dense_model = SentenceTransformer(\n",
    "    DENSE_MODEL_NAME,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "DENSE_VECTOR_SIZE = dense_model.get_sentence_embedding_dimension()\n",
    "print(\"Dense model \" + DENSE_MODEL_NAME + \" initialized\")\n",
    "\n",
    "# Sparse embedder\n",
    "SPARSE_MODEL_NAME = \"naver/splade_v2_max\"\n",
    "sparse_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    SPARSE_MODEL_NAME,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "sparse_model = AutoModel.from_pretrained(SPARSE_MODEL_NAME).to(DEVICE)\n",
    "SPARSE_VECTOR_SIZE = 50000  # Sparse embeddings can get very large\n",
    "print(\"Sparse model \" + SPARSE_MODEL_NAME + \" initialized\")\n",
    "# Choices:\n",
    "#    TF-IDF: Term frequency based\n",
    "#    BM25: Probabilistic-based\n",
    "#    SPLADE: Hybrid dense-sparse\n",
    "\n",
    "# Database\n",
    "COLLECTION_NAME = \"class_materials\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#### Reading Files ####\n",
    "'''\n",
    "Reads the file at filepath and splits it into chunks.\n",
    "\n",
    "Args:\n",
    "    filepath (String): path to file to be read\n",
    "Returns:\n",
    "    array of Document objects, each a chunk of file read\n",
    "'''\n",
    "def read_file(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    docs = [Document(page_content=content)]\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "    # Add title to each chunk to use as id in database\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk.metadata[\"title\"] = f\"{filepath}_{i}\"\n",
    "        print(chunk.page_content)\n",
    "        print('---')\n",
    "        \n",
    "    file.close()\n",
    "\n",
    "    return chunks"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### pip installs ####\n",
    "%pip install langchain-experimental langchain-huggingface qdrant-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Imports ####\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Reading Files #\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Embeddings #\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Database #\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initializations ####\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using \" + DEVICE)\n",
    "\n",
    "# Text splitter\n",
    "# Use a chunker to chunk text\n",
    "# Different options here: length chunking, character break chunking, semantic chunking\n",
    "# Using semantic chunking for best separation of different information to help retrieval\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "text_splitter = SemanticChunker(embeddings=embeddings)\n",
    "\n",
    "# Dense embedder\n",
    "DENSE_MODEL_NAME = \"intfloat/e5-base\"\n",
    "dense_model = SentenceTransformer(\n",
    "    DENSE_MODEL_NAME,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "DENSE_VECTOR_SIZE = dense_model.get_sentence_embedding_dimension()\n",
    "print(\"Dense model \" + DENSE_MODEL_NAME + \" initialized\")\n",
    "\n",
    "# Sparse embedder\n",
    "SPARSE_MODEL_NAME = \"naver/splade_v2_max\"\n",
    "sparse_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    SPARSE_MODEL_NAME,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "sparse_model = AutoModel.from_pretrained(SPARSE_MODEL_NAME).to(DEVICE)\n",
    "SPARSE_VECTOR_SIZE = 50000  # Sparse embeddings can get very large\n",
    "print(\"Sparse model \" + SPARSE_MODEL_NAME + \" initialized\")\n",
    "# Choices:\n",
    "#    TF-IDF: Term frequency based\n",
    "#    BM25: Probabilistic-based\n",
    "#    SPLADE: Hybrid dense-sparse\n",
    "\n",
    "# Database\n",
    "COLLECTION_NAME = \"class_materials\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Reading Files ####\n",
    "'''\n",
    "Reads the file at filepath and splits it into chunks.\n",
    "\n",
    "Args:\n",
    "    filepath (String): path to file to be read\n",
    "Returns:\n",
    "    array of Document objects, each a chunk of file read\n",
    "'''\n",
    "def read_file(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    docs = [Document(page_content=content)]\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "    # Add title to each chunk to use as id in database\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk.metadata[\"title\"] = f\"{filepath}_{i}\"\n",
    "        print(chunk.page_content)\n",
    "        print('---')\n",
    "        \n",
    "    file.close()\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Embeddings ####\n",
    "'''\n",
    "Sparse embeddings using E5-base model\n",
    "\n",
    "Args:\n",
    "    text (String): string to be embedded\n",
    "Returns:\n",
    "    sparse embedding of text\n",
    "'''\n",
    "def sparse_embed(text):\n",
    "    inputs = sparse_tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = sparse_model(**inputs)\n",
    "\n",
    "    sparse_embedding = torch.log(1 + torch.relu(outputs.last_hidden_state).sum(dim=1))\n",
    "    return sparse_embedding.squeeze().numpy()\n",
    "\n",
    "'''\n",
    "Dense embeddings\n",
    "\n",
    "Args:\n",
    "    text (String): string to be embedded\n",
    "Returns:\n",
    "    dense embedding of text\n",
    "'''\n",
    "def dense_embed(text):\n",
    "    # Format required for E5\n",
    "    formatted_text = f\"query: {text}\"\n",
    "    return dense_model.encode(formatted_text, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Vectorizing Chunks ####\n",
    "'''\n",
    "Concatenate sparse and dense embedding vectors and return combined vector\n",
    "\n",
    "Args:\n",
    "    sparse_embedding (numpy vector)\n",
    "    dense_embedding (numpy vector)\n",
    "Returns:\n",
    "    vectors of each chunk\n",
    "'''\n",
    "def vectorize(text):\n",
    "    sparse_embedding = sparse_embed(text)\n",
    "    dense_embedding = dense_embed(text)\n",
    "    combined_vector = np.concatenate((sparse_embedding, dense_embedding))\n",
    "    return combined_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Database ####\n",
    "'''\n",
    "Establishes a connection to Qdrant database\n",
    "\n",
    "Args:\n",
    "    none\n",
    "Returns:\n",
    "    QdrantClient object with connection to database\n",
    "'''\n",
    "def start_database(recreate):\n",
    "    client = QdrantClient(\n",
    "        url=\"https://07de6745-b3ea-4156-9daf-a4cbbb339b92.us-east4-0.gcp.cloud.qdrant.io:6333\", \n",
    "        api_key=\"<your-token>\",\n",
    "    )\n",
    "\n",
    "    # If recreate=True, recreate the collection\n",
    "    if recreate:\n",
    "        client.recreate_collection(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            vectors_config={\n",
    "                \"sparse_vector\": VectorParams(size=SPARSE_VECTOR_SIZE, distance=Distance.DOT),\n",
    "                \"dense_vector\": VectorParams(size=DENSE_VECTOR_SIZE, distance=Distance.COSINE),\n",
    "            },\n",
    "        )\n",
    "\n",
    "    # Create collection if it doesn't exist\n",
    "    if not client.collection_exists(COLLECTION_NAME):\n",
    "        client.create_collection(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            vectors_config={\n",
    "                \"sparse_vector\": VectorParams(size=SPARSE_VECTOR_SIZE, distance=Distance.DOT),\n",
    "                \"dense_vector\": VectorParams(size=DENSE_VECTOR_SIZE, distance=Distance.COSINE),\n",
    "            },\n",
    "        )\n",
    "    \n",
    "    print(client.get_collections())\n",
    "\n",
    "    return client\n",
    "\n",
    "'''\n",
    "Upserts vectors of text chunks to database\n",
    "\n",
    "Args:\n",
    "    client (QdrantClient): client object with connection to database\n",
    "    chunks ([Document]): array of Document objects to upsert\n",
    "'''\n",
    "def upsert(client, chunks):\n",
    "    # Vectorize each chunk of text\n",
    "    vectors = [vectorize(chunk.page_content) for chunk in chunks]\n",
    "\n",
    "    # Upsert dense and sparse vectors to database\n",
    "    client.upsert(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points=[\n",
    "            {\n",
    "                \"id\": chunk.metadata[\"title\"],\n",
    "                \"vector\": {\"sparse_vector\": vectors[i][0], \"dense_vector\": vectors[i][1]},\n",
    "                \"payload\": {\"text\": chunk.page_content},\n",
    "            }\n",
    "            for i, chunk in enumerate(chunks)\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Context Retrieval ####\n",
    "'''\n",
    "Perform a similarity search on sparse embeddings\n",
    "\n",
    "Args:\n",
    "    client (QdrantClient): client object with connection to database\n",
    "    query_sparse: sparse embedding of query\n",
    "    top_k: number of top results to return\n",
    "Returns:\n",
    "    top_k results from similarity search on sparse embeddings\n",
    "'''\n",
    "def sparse_query(client, query_sparse, top_k):\n",
    "    results = client.search(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query_vector=query_sparse,\n",
    "        limit=top_k,\n",
    "        vector_type=\"sparse\",\n",
    "        with_payload=\"True\",\n",
    "    )\n",
    "    return results\n",
    "\n",
    "'''\n",
    "Perform a similarity search on dense embeddings\n",
    "\n",
    "Args:\n",
    "    client (QdrantClient): client object with connection to database\n",
    "    query_dense: dense embedding of query\n",
    "    top_k: number of top results to return\n",
    "Returns:\n",
    "    top_k results from similarity search on dense embeddings\n",
    "'''\n",
    "def dense_query(client, query_dense, top_k):\n",
    "    results = client.search(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query_vector=query_dense,\n",
    "        limit=top_k,\n",
    "        vector_type=\"dense\",\n",
    "        with_payload=\"True\",\n",
    "    )\n",
    "    return results\n",
    "\n",
    "'''\n",
    "Calculates a weighted score for each query result\n",
    "\n",
    "Args:\n",
    "    sparse_score: similarity score of sparse embedding\n",
    "    dense_score: similarity score of dense embedding\n",
    "Returns:\n",
    "    weighted score combining both sparse and dense scores\n",
    "'''\n",
    "def weighted_score(sparse_score, dense_score, sparse_weight=0.3, dense_weight=0.7):\n",
    "    return (sparse_weight * sparse_score) + (dense_weight * dense_score)\n",
    "\n",
    "'''\n",
    "Combines sparse and dense query results\n",
    "'''\n",
    "\n",
    "def combine_queries(sparse_results, dense_results):\n",
    "    # Gather scores for all results\n",
    "    all_results = {}\n",
    "    for dense in dense_results:\n",
    "        all_results[dense.id] = {\"dense_score\": dense.score, \"sparse_score\": 0, \"text\": dense.payload[\"text\"]}\n",
    "\n",
    "    for sparse in sparse_results:\n",
    "        if sparse.id not in all_results:\n",
    "            all_results[sparse.id] = {\"dense_score\": 0, \"sparse_score\": sparse.score, \"text\": sparse.payload[\"text\"]}\n",
    "        else:\n",
    "            all_results[sparse.id][\"sparse_score\"] = sparse.score\n",
    "\n",
    "    # Weighted results\n",
    "    scored_results = {}\n",
    "    for result in all_results:\n",
    "        sparse_score = all_results[result][\"sparse_score\"]\n",
    "        dense_score = all_results[result][\"dense_score\"]\n",
    "        scored_results[result] = {\n",
    "            \"score\": weighted_score(sparse_score, dense_score),\n",
    "            \"text\": all_results[result][\"text\"],\n",
    "        }\n",
    "        \n",
    "    return scored_results\n",
    "\n",
    "'''\n",
    "Perform a hybrid query (sparse and dense) on vector database to provide as context to llm\n",
    "\n",
    "Args:\n",
    "    client (QdrantClient): client object with connection to database\n",
    "    query (String): input from user asked to LLM\n",
    "Returns:\n",
    "    list of text to serve as context for LLM\n",
    "'''\n",
    "def hybrid_query(client, query, top_k):\n",
    "    # Vectorize the query\n",
    "    query_vector = vectorize(query)\n",
    "    query_sparse = query_vector[0]\n",
    "    query_dense = query_vector[1]\n",
    "    \n",
    "    # Query database using both sparse and dense embeddings\n",
    "    sparse_results = sparse_query(client, query_sparse, top_k)\n",
    "    dense_results = dense_query(client, query_dense, top_k)\n",
    "\n",
    "    # Combine and calculate weighted scores for all results\n",
    "    scored_results = combine_queries(sparse_results, dense_results)\n",
    "\n",
    "    # Sort in descending order by combined score\n",
    "    sorted_results = sorted(scored_results, key=lambda x: x[\"score\"], reverse=True)\n",
    "    \n",
    "    # Return results\n",
    "    text_results = []\n",
    "    for i, result in enumerate(sorted_results):\n",
    "        if i == top_k:\n",
    "            break\n",
    "        text_results.append(sorted_results[result][\"text\"])\n",
    "        \n",
    "    return text_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Main Function ####\n",
    "client = start_database(recreate=True)\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        filepath = os.path.join(dirname, filename)\n",
    "        print(filepath)\n",
    "        chunks = read_file(filepath)\n",
    "        upsert(chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
